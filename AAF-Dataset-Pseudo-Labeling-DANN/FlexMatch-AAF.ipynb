{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FairFace Dataset as pseudo-labeling, evaluated on All-Age-Face Data (AAF - Mostly Asian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics in c:\\users\\haohu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy>1.20.0 in c:\\users\\haohu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchmetrics) (1.26.2)\n",
      "Requirement already satisfied: packaging>17.1 in c:\\users\\haohu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchmetrics) (24.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\haohu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchmetrics) (2.5.1+cu121)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in c:\\users\\haohu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchmetrics) (0.11.9)\n",
      "Requirement already satisfied: setuptools in c:\\users\\haohu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (69.2.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\haohu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\haohu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.15.4)\n",
      "Requirement already satisfied: networkx in c:\\users\\haohu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\haohu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\haohu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (2023.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\haohu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\haohu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\haohu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=2.0.0->torchmetrics) (2.1.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b76ab2380efd4835b31af9d6548b5e17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86744 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b0c0335c3847ca98690b73fedcdffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10954 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788748f35ff749af87771f5db029d5df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86744 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732439b48e2d4301ba52211feb397ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10954 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82823cd16f94f7ab06bbd058acdb0d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10954 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f274c188141b40ce9a28a54eb876d294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10954 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from timm import create_model\n",
    "!pip install torchmetrics\n",
    "from torchmetrics.classification import BinaryAccuracy, Accuracy\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "base_img = Path('../Pseudo-Labeling/FairFace')\n",
    "\n",
    "train_df = pd.read_csv('../Pseudo-Labeling/FairFace/train_labels.csv')\n",
    "val_df = pd.read_csv('../Pseudo-Labeling/FairFace/val_labels.csv')\n",
    "\n",
    "train_df.drop(columns=['service_test'], inplace=True)\n",
    "val_df.drop(columns=['service_test'], inplace=True)\n",
    "\n",
    "train_df['file'] = train_df['file'].progress_map(lambda x: base_img / x)\n",
    "val_df['file'] = val_df['file'].progress_map(lambda x: base_img / x)\n",
    "\n",
    "train_df['age'] = train_df['age'].astype('category')\n",
    "train_df['age_code'] = train_df['age'].cat.codes\n",
    "\n",
    "age_map = dict(zip(train_df['age'].cat.categories, range(len(train_df['age'].cat.categories))))\n",
    "gender_map = {'Male': 0, 'Female': 1}\n",
    "\n",
    "train_df['gender_code'] = train_df['gender'].progress_map(lambda x: gender_map[x])\n",
    "val_df['age_code'] = val_df['age'].progress_map(lambda x: age_map[x])\n",
    "val_df['gender_code'] = val_df['gender'].progress_map(lambda x: gender_map[x])\n",
    "\n",
    "# Encoding race categories\n",
    "train_df['race'] = train_df['race'].astype('category')\n",
    "train_df['race_code'] = train_df['race'].cat.codes\n",
    "\n",
    "race_map = dict(zip(train_df['race'].cat.categories, range(len(train_df['race'].cat.categories))))\n",
    "val_df['race_code'] = val_df['race'].progress_map(lambda x: race_map[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FairFace Data Retrieval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from timm import create_model\n",
    "from torchmetrics.classification import BinaryAccuracy, Accuracy\n",
    "import os\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Define directories for FairFace dataset\n",
    "dataset_path_orig = Path(\"FairFace\")\n",
    "dataset_path_dest = Path(\"./fairface_race_dataset\")\n",
    "os.makedirs(dataset_path_dest, exist_ok=True)\n",
    "\n",
    "dataset_path_training = dataset_path_dest / \"Training\"\n",
    "dataset_path_validation = dataset_path_dest / \"Validation\"\n",
    "os.makedirs(dataset_path_training, exist_ok=True)\n",
    "os.makedirs(dataset_path_validation, exist_ok=True)\n",
    "\n",
    "dataset_path_training_female = dataset_path_training / \"female\"\n",
    "dataset_path_training_male = dataset_path_training / \"male\"\n",
    "dataset_path_validation_female = dataset_path_validation / \"female\"\n",
    "dataset_path_validation_male = dataset_path_validation / \"male\"\n",
    "os.makedirs(dataset_path_training_female, exist_ok=True)\n",
    "os.makedirs(dataset_path_training_male, exist_ok=True)\n",
    "os.makedirs(dataset_path_validation_female, exist_ok=True)\n",
    "os.makedirs(dataset_path_validation_male, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter FairFace dataset by gender and age\n",
    "for index, row in train_df.iterrows():\n",
    "    img_path_orig = row['file']\n",
    "    gender = row['gender_code']\n",
    "\n",
    "    if gender == 1:  # Female\n",
    "        img_path_dest = dataset_path_training_female / img_path_orig.name\n",
    "    elif gender == 0:  # Male\n",
    "        img_path_dest = dataset_path_training_male / img_path_orig.name\n",
    "    else:\n",
    "        print('Weird things happening')\n",
    "\n",
    "    shutil.copy(str(img_path_orig), str(img_path_dest))\n",
    "\n",
    "for index, row in val_df.iterrows():\n",
    "    img_path_orig = row['file']\n",
    "    gender = row['gender_code']\n",
    "\n",
    "    if gender == 1:  # Female\n",
    "        img_path_dest = dataset_path_validation_female / img_path_orig.name\n",
    "    elif gender == 0:  # Male\n",
    "        img_path_dest = dataset_path_validation_male / img_path_orig.name\n",
    "    else:\n",
    "        print('Weird things happening')\n",
    "\n",
    "    shutil.copy(str(img_path_orig), str(img_path_dest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'0-2': 0,\n",
       "  '10-19': 1,\n",
       "  '20-29': 2,\n",
       "  '3-9': 3,\n",
       "  '30-39': 4,\n",
       "  '40-49': 5,\n",
       "  '50-59': 6,\n",
       "  '60-69': 7,\n",
       "  'more than 70': 8},\n",
       " {'Male': 0, 'Female': 1},\n",
       " {'Black': 0,\n",
       "  'East Asian': 1,\n",
       "  'Indian': 2,\n",
       "  'Latino_Hispanic': 3,\n",
       "  'Middle Eastern': 4,\n",
       "  'Southeast Asian': 5,\n",
       "  'White': 6})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_map, gender_map, race_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age_code</th>\n",
       "      <th>gender_code</th>\n",
       "      <th>race_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>..\\Pseudo-Labeling\\FairFace\\val\\1.jpg</td>\n",
       "      <td>3-9</td>\n",
       "      <td>Male</td>\n",
       "      <td>East Asian</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>..\\Pseudo-Labeling\\FairFace\\val\\2.jpg</td>\n",
       "      <td>50-59</td>\n",
       "      <td>Female</td>\n",
       "      <td>East Asian</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>..\\Pseudo-Labeling\\FairFace\\val\\3.jpg</td>\n",
       "      <td>30-39</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>..\\Pseudo-Labeling\\FairFace\\val\\4.jpg</td>\n",
       "      <td>20-29</td>\n",
       "      <td>Female</td>\n",
       "      <td>Latino_Hispanic</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>..\\Pseudo-Labeling\\FairFace\\val\\5.jpg</td>\n",
       "      <td>20-29</td>\n",
       "      <td>Male</td>\n",
       "      <td>Southeast Asian</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    file    age  gender             race  \\\n",
       "0  ..\\Pseudo-Labeling\\FairFace\\val\\1.jpg    3-9    Male       East Asian   \n",
       "1  ..\\Pseudo-Labeling\\FairFace\\val\\2.jpg  50-59  Female       East Asian   \n",
       "2  ..\\Pseudo-Labeling\\FairFace\\val\\3.jpg  30-39    Male            White   \n",
       "3  ..\\Pseudo-Labeling\\FairFace\\val\\4.jpg  20-29  Female  Latino_Hispanic   \n",
       "4  ..\\Pseudo-Labeling\\FairFace\\val\\5.jpg  20-29    Male  Southeast Asian   \n",
       "\n",
       "   age_code  gender_code  race_code  \n",
       "0         3            0          1  \n",
       "1         6            1          1  \n",
       "2         4            0          6  \n",
       "3         2            1          3  \n",
       "4         2            0          5  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pictures in each training folder:\n",
      "male: 45986\n",
      "female: 40758\n",
      "\n",
      "Number of pictures in each validation folder:\n",
      "male: 5792\n",
      "female: 5162\n"
     ]
    }
   ],
   "source": [
    "data_dir =  \"./fairface_race_dataset\"\n",
    "\n",
    "# Define the folders in the training and validation directories\n",
    "train_folders = ['male', 'female']\n",
    "val_folders = ['male', 'female']\n",
    "\n",
    "# Function to count the number of pictures in each folder\n",
    "def count_pictures(folder_path):\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        count += len(files)\n",
    "    return count\n",
    "\n",
    "# Print the number of pictures in each training folder\n",
    "print(\"Number of pictures in each training folder:\")\n",
    "for folder in train_folders:\n",
    "    folder_path = os.path.join(data_dir, 'Training', folder)\n",
    "    num_pics = count_pictures(folder_path)\n",
    "    print(f\"{folder}: {num_pics}\")\n",
    "\n",
    "# Print the number of pictures in each validation folder\n",
    "print(\"\\nNumber of pictures in each validation folder:\")\n",
    "for folder in val_folders:\n",
    "    folder_path = os.path.join(data_dir, 'Validation', folder)\n",
    "    num_pics = count_pictures(folder_path)\n",
    "    print(f\"{folder}: {num_pics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "12.1\n",
      "CUDA Available: True\n",
      "Device Name: NVIDIA GeForce GTX 1650 Ti\n",
      "CUDA Version: 12.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # device object\n",
    "print(device)\n",
    "print(torch.version.cuda)\n",
    "\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA Version:\", torch.version.cuda)\n",
    "else:\n",
    "    print(\"CUDA not detected.\")\n",
    "\n",
    "random.seed(123)\n",
    "\n",
    "# change to cpu\n",
    "# device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully and saved as classification_model.pth\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Define the URL for the file\n",
    "url = \"https://postechackr-my.sharepoint.com/:u:/g/personal/dongbinna_postech_ac_kr/EVd9bFWzqztMrXRDdNnCHQkBsHaM4n5_1q1fue77vtQVtw?download=1\"\n",
    "\n",
    "# Define the path where the file will be saved\n",
    "output_path = \"classification_model.pth\"\n",
    "\n",
    "# Send a GET request to download the file\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    with open(output_path, \"wb\") as file:\n",
    "        file.write(response.content)  # Write the content of the response to the file\n",
    "    print(f\"File downloaded successfully and saved as {output_path}\")\n",
    "else:\n",
    "    print(f\"Failed to download file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haohu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\haohu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\haohu\\AppData\\Local\\Temp\\ipykernel_13920\\2580958553.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(pretrained_weights_path, map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained ResNet model CNN-based Face-Gender-Classification PyTorch model \n",
    "model = models.resnet18(pretrained=True)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 2)  # Output layer for binary classification\n",
    "\n",
    "# Load pre-trained weights from .pth file\n",
    "pretrained_weights_path = 'classification_model.pth'\n",
    "model.load_state_dict(torch.load(pretrained_weights_path, map_location=torch.device('cpu')))\n",
    "\n",
    "# Send model to device\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "### Dataset to be Evaluated\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # device object\n",
    "print(device)\n",
    "random.seed(123)\n",
    "\n",
    "# Dataset 3\n",
    "dataset_path_sets = Path(\"../Pseudo-Labeling/gender_classification_dataset_asia_raw/All-Age-Faces Dataset/image sets\")\n",
    "dataset_path_orig = Path(\"../Pseudo-Labeling/gender_classification_dataset_asia_raw/All-Age-Faces Dataset/original images\")\n",
    "dataset_path_dest = Path(\"./aaf_dataset\")\n",
    "os.makedirs(dataset_path_dest, exist_ok=True)\n",
    "\n",
    "dataset_path_training = dataset_path_dest / \"Training\"\n",
    "dataset_path_validation = dataset_path_dest / \"Validation\"\n",
    "os.makedirs(dataset_path_training, exist_ok=True)\n",
    "os.makedirs(dataset_path_validation, exist_ok=True)\n",
    "\n",
    "dataset_path_training_female = dataset_path_training / \"female\"\n",
    "dataset_path_training_male = dataset_path_training / \"male\"\n",
    "dataset_path_validation_female = dataset_path_validation / \"female\"\n",
    "dataset_path_validation_male = dataset_path_validation / \"male\"\n",
    "os.makedirs(dataset_path_training_female, exist_ok=True)\n",
    "os.makedirs(dataset_path_training_male, exist_ok=True)\n",
    "os.makedirs(dataset_path_validation_female, exist_ok=True)\n",
    "os.makedirs(dataset_path_validation_male, exist_ok=True)\n",
    "\n",
    "# Dataset 3\n",
    "# https://pythonawesome.com/all-age-faces-dataset-mostly-asian/\n",
    "# From original dataset webpage:\n",
    "# Individuals from serial number 00000 to 07380 are all female, from 07381 to 13321 are male.\n",
    "# Typical code: 00013A02.jpg --> image 00013, Age 02 years old\n",
    "cols = ['img', 'label']\n",
    "df_train = pd.read_csv(dataset_path_sets / \"train.txt\", sep=' ', header=None, names=cols)\n",
    "df_val = pd.read_csv(dataset_path_sets / \"val.txt\", sep=' ', header=None, names=cols)\n",
    "\n",
    "for idx, row in df_train.iterrows():\n",
    "  img_name = row['img']\n",
    "  label = row['label']\n",
    "  img_path_orig = dataset_path_orig / img_name\n",
    "  img_num = img_name[:5]\n",
    "  img_age = img_name[6:8]\n",
    "  if int(img_age) >= 18: # Only consider adults\n",
    "    if label == 0:\n",
    "      img_path_dest = dataset_path_training_female / img_path_orig.name\n",
    "    elif label == 1:\n",
    "      img_path_dest = dataset_path_training_male / img_path_orig.name\n",
    "    else:\n",
    "      print('Weird things happening')\n",
    "\n",
    "    shutil.copy(str(img_path_orig), str(img_path_dest))\n",
    "\n",
    "for idx, row in df_val.iterrows():\n",
    "  img_name = row['img']\n",
    "  label = row['label']\n",
    "  img_path_orig = dataset_path_orig / img_name\n",
    "  img_num = img_name[:5]\n",
    "  img_age = img_name[6:8]\n",
    "  if int(img_age) >= 18:\n",
    "    if label == 0:\n",
    "      img_path_dest = dataset_path_validation_female / img_path_orig.name\n",
    "    elif label == 1:\n",
    "      img_path_dest = dataset_path_validation_male / img_path_orig.name\n",
    "    else:\n",
    "      print('Weird things happening')\n",
    "\n",
    "    shutil.copy(str(img_path_orig), str(img_path_dest))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Unlabeled dataset prepared in: ./aaf_dataset\\Training/Unlabeled\n",
      "Training Number of images in the unlabeled dataset: 5654\n",
      "Unlabeled dataset prepared in: ./aaf_dataset\\Unlabeled\n",
      "Number of images in the unlabeled dataset: 11345\n",
      "Evaluation dataset available at: ./fairface_race_dataset\n",
      "Number of pictures in each training folder:\n",
      "male: 45986\n",
      "female: 40758\n",
      "\n",
      "Number of pictures in each validation folder:\n",
      "male: 5792\n",
      "female: 5162\n"
     ]
    }
   ],
   "source": [
    "#CREATE UNLABELED DATALOADER FROM THE TRAINING SPLIT\n",
    "#From training dataset, take the data out of \"female\" and \"male\" folders\n",
    "import os\n",
    "from shutil import copy\n",
    "\n",
    "data_dir = \"./aaf_dataset\" \n",
    "fairface_data_dir = \"./fairface_race_dataset\"\n",
    "\n",
    "male_folder = \"aaf_dataset/Training/male\"\n",
    "female_folder = \"aaf_dataset/Training/female\" \n",
    "\n",
    "male_folder_val = \"aaf_dataset/Validation/male\"\n",
    "female_folder_val = \"aaf_dataset/Validation/female\"\n",
    "\n",
    "unlabeled_folder = os.path.join(data_dir, \"Unlabeled\")\n",
    "unlabeled_training_folder = os.path.join(data_dir, \"Training/Unlabeled\")\n",
    "os.makedirs(unlabeled_folder, exist_ok=True)\n",
    "os.makedirs(unlabeled_training_folder, exist_ok=True)\n",
    "# Copy all files from male and female subfolders to the unlabeled folder\n",
    "for folder in [male_folder, female_folder]:\n",
    "    for filename in os.listdir(folder):\n",
    "        source_path = os.path.join(folder, filename)\n",
    "        destination_path = os.path.join(unlabeled_training_folder, filename)\n",
    "        copy(source_path, destination_path)\n",
    "\n",
    "# Copy all files from male and female subfolders to the unlabeled folder\n",
    "for folder in [male_folder_val, female_folder_val]:\n",
    "    for filename in os.listdir(folder):\n",
    "        source_path = os.path.join(folder, filename)\n",
    "        destination_path = os.path.join(unlabeled_folder, filename)\n",
    "        copy(source_path, destination_path)\n",
    "# Copy all files from male and female subfolders to the unlabeled folder\n",
    "for folder in [male_folder, female_folder]:\n",
    "    for filename in os.listdir(folder):\n",
    "        source_path = os.path.join(folder, filename)\n",
    "        destination_path = os.path.join(unlabeled_folder, filename)\n",
    "        copy(source_path, destination_path)\n",
    "\n",
    "print(f\"Training Unlabeled dataset prepared in: {unlabeled_training_folder}\")\n",
    "print(f\"Training Number of images in the unlabeled dataset: {len(os.listdir(unlabeled_training_folder))}\")\n",
    "\n",
    "print(f\"Unlabeled dataset prepared in: {unlabeled_folder}\")\n",
    "print(f\"Number of images in the unlabeled dataset: {len(os.listdir(unlabeled_folder))}\")\n",
    "\n",
    "\n",
    "# The Asia dataset remains intact for evaluation\n",
    "evaluation_data_dir = fairface_data_dir\n",
    "print(f\"Evaluation dataset available at: {evaluation_data_dir}\")\n",
    "\n",
    "# Define the folders in the training and validation directories\n",
    "train_folders = ['male', 'female']\n",
    "val_folders = ['male', 'female']\n",
    "\n",
    "# Function to count the number of pictures in each folder\n",
    "def count_pictures(folder_path):\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        count += len(files)\n",
    "    return count\n",
    "\n",
    "# Print the number of pictures in each training folder\n",
    "print(\"Number of pictures in each training folder:\")\n",
    "for folder in train_folders:\n",
    "    folder_path = os.path.join(evaluation_data_dir, 'Training', folder)\n",
    "    num_pics = count_pictures(folder_path)\n",
    "    print(f\"{folder}: {num_pics}\")\n",
    "\n",
    "# Print the number of pictures in each validation folder\n",
    "print(\"\\nNumber of pictures in each validation folder:\")\n",
    "for folder in val_folders:\n",
    "    folder_path = os.path.join(evaluation_data_dir, 'Validation', folder)\n",
    "    num_pics = count_pictures(folder_path)\n",
    "    print(f\"{folder}: {num_pics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformations\n",
    "transforms_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transforms_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transforms_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  43715 KiB |  43715 KiB |  43715 KiB |      0 B   |\n",
      "|       from large pool |  40320 KiB |  40320 KiB |  40320 KiB |      0 B   |\n",
      "|       from small pool |   3395 KiB |   3395 KiB |   3395 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  43715 KiB |  43715 KiB |  43715 KiB |      0 B   |\n",
      "|       from large pool |  40320 KiB |  40320 KiB |  40320 KiB |      0 B   |\n",
      "|       from small pool |   3395 KiB |   3395 KiB |   3395 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  43699 KiB |  43699 KiB |  43699 KiB |      0 B   |\n",
      "|       from large pool |  40320 KiB |  40320 KiB |  40320 KiB |      0 B   |\n",
      "|       from small pool |   3379 KiB |   3379 KiB |   3379 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  65536 KiB |  65536 KiB |  65536 KiB |      0 B   |\n",
      "|       from large pool |  61440 KiB |  61440 KiB |  61440 KiB |      0 B   |\n",
      "|       from small pool |   4096 KiB |   4096 KiB |   4096 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  21820 KiB |  21833 KiB |  45339 KiB |  23518 KiB |\n",
      "|       from large pool |  21120 KiB |  21120 KiB |  41856 KiB |  20736 KiB |\n",
      "|       from small pool |    700 KiB |   2011 KiB |   3483 KiB |   2782 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     122    |     122    |     122    |       0    |\n",
      "|       from large pool |       8    |       8    |       8    |       0    |\n",
      "|       from small pool |     114    |     114    |     114    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     122    |     122    |     122    |       0    |\n",
      "|       from large pool |       8    |       8    |       8    |       0    |\n",
      "|       from small pool |     114    |     114    |     114    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       5    |       5    |       5    |       0    |\n",
      "|       from large pool |       3    |       3    |       3    |       0    |\n",
      "|       from small pool |       2    |       2    |       2    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       5    |       5    |       5    |       0    |\n",
      "|       from large pool |       3    |       3    |       3    |       0    |\n",
      "|       from small pool |       2    |       2    |       2    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# clean cache\n",
    "torch.cuda.empty_cache()\n",
    "# check if the cache is empty\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "# check if cuda is available\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6244d75661649eaa443abd7b5de34f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Copying files to ./fairface_race_dataset\\East_Asian_Validation:   0%|          | 0/1550 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "East Asian Validation Set created with 1550 samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9162a320045b4495b56920dcdb8cc89d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Copying files to ./fairface_race_dataset\\Balanced_Validation:   0%|          | 0/8463 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Validation Set created with 8463 samples.\n",
      "Dataset splitting complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Define the paths for validation and training\n",
    "data_dir = \"./fairface_race_dataset\"  \n",
    "validation_path = os.path.join(data_dir, 'Validation')\n",
    "training_path = os.path.join(data_dir, 'Training')\n",
    "east_asian_val_path = os.path.join(data_dir, 'East_Asian_Validation')\n",
    "balanced_val_path = os.path.join(data_dir, 'Balanced_Validation')\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(east_asian_val_path, exist_ok=True)\n",
    "os.makedirs(balanced_val_path, exist_ok=True)\n",
    "\n",
    "# Load metadata\n",
    "validation_data = pd.read_csv(\"../Pseudo-Labeling/FairFace/val_labels.csv\")\n",
    "training_data = pd.read_csv(\"../Pseudo-Labeling/FairFace/train_labels.csv\")\n",
    "\n",
    "\n",
    "# Function to copy files and separate by gender\n",
    "def copy_files(file_list, gender_list, source_dir, dest_dir):\n",
    "    for file, gender in tqdm(zip(file_list, gender_list), desc=f\"Copying files to {dest_dir}\", total=len(file_list)):\n",
    "        gender_folder = \"male\" if gender == \"Male\" else \"female\"\n",
    "        file_path = os.path.normpath(os.path.join(source_dir, gender_folder, os.path.basename(file)))\n",
    "        dst_dir = os.path.join(dest_dir, gender_folder)\n",
    "        os.makedirs(dst_dir, exist_ok=True)  # Create gender-specific folder if it doesn't exist\n",
    "        dst = os.path.join(dst_dir, os.path.basename(file))\n",
    "        if os.path.exists(file_path):\n",
    "            shutil.copy(file_path, dst)\n",
    "        else:\n",
    "            print(f\"Warning: {file_path} does not exist.\")\n",
    "\n",
    "# Create East Asian validation set\n",
    "east_asian_val = validation_data[validation_data['race'] == 'East Asian']\n",
    "copy_files(east_asian_val['file'], east_asian_val['gender'], validation_path, east_asian_val_path)\n",
    "print(f\"East Asian Validation Set created with {len(east_asian_val)} samples.\")\n",
    "\n",
    "# Create a balanced validation set across all races\n",
    "min_samples_per_race = validation_data['race'].value_counts().min()\n",
    "balanced_val = validation_data.groupby('race').apply(lambda x: x.sample(min_samples_per_race, random_state=42)).reset_index(drop=True)\n",
    "copy_files(balanced_val['file'], balanced_val['gender'], validation_path, balanced_val_path)\n",
    "print(f\"Balanced Validation Set created with {len(balanced_val)} samples.\")\n",
    "\n",
    "print(\"Dataset splitting complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in Validation - Female: 5162\n",
      "Number of samples in Validation - Male: 5792\n",
      "Total samples in Validation: 10954\n",
      "Number of samples in East - Female: 773\n",
      "Number of samples in East - Male: 777\n",
      "Total samples in East: 1550\n",
      "Number of samples in Balanced - Female: 3935\n",
      "Number of samples in Balanced - Male: 4528\n",
      "Total samples in Balanced: 8463\n"
     ]
    }
   ],
   "source": [
    "#PRINT INFO\n",
    "import os\n",
    "\n",
    "evaluation_dataset_path = \"./fairface_race_dataset\"\n",
    "training_folder = os.path.join(evaluation_dataset_path, \"Training\")\n",
    "validation_folder = os.path.join(evaluation_dataset_path, \"Validation\")\n",
    "east_asian_val_folder = os.path.join(evaluation_dataset_path, \"East_Asian_Validation\")\n",
    "balanced_val_folder = os.path.join(evaluation_dataset_path, \"Balanced_Validation\")\n",
    "\n",
    "# Define the paths to the female and male folders within the validation folder\n",
    "validation_female_folder = os.path.join(validation_folder, \"female\")\n",
    "validation_male_folder = os.path.join(validation_folder, \"male\")\n",
    "\n",
    "# Define the paths to the east asian validation folders\n",
    "east_asian_female_folder = os.path.join(east_asian_val_folder, \"female\")\n",
    "east_asian_male_folder = os.path.join(east_asian_val_folder, \"male\")\n",
    "\n",
    "# Define the paths to the balanced validation folders\n",
    "balanced_female_folder = os.path.join(balanced_val_folder, \"female\")\n",
    "balanced_male_folder = os.path.join(balanced_val_folder, \"male\")\n",
    "\n",
    "# Count the number of samples in each folder\n",
    "num_validation_female_samples = len(os.listdir(validation_female_folder))\n",
    "num_validation_male_samples = len(os.listdir(validation_male_folder))\n",
    "num_east_asian_female_samples = len(os.listdir(east_asian_female_folder))\n",
    "num_east_asian_male_samples = len(os.listdir(east_asian_male_folder))\n",
    "balanced_female_samples =  len(os.listdir(balanced_female_folder))\n",
    "balanced_male_samples = len(os.listdir(balanced_male_folder))\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the totals\n",
    "total_validation_samples = num_validation_female_samples + num_validation_male_samples\n",
    "total_east_asian_samples = num_east_asian_female_samples + num_east_asian_male_samples\n",
    "total_balanced = balanced_female_samples + balanced_male_samples\n",
    "\n",
    "\n",
    "print(\"Number of samples in Validation - Female:\", num_validation_female_samples)\n",
    "print(\"Number of samples in Validation - Male:\", num_validation_male_samples)\n",
    "print(\"Total samples in Validation:\", total_validation_samples)\n",
    "print(\"Number of samples in East - Female:\", num_east_asian_female_samples)\n",
    "print(\"Number of samples in East - Male:\", num_east_asian_male_samples)\n",
    "print(\"Total samples in East:\", total_east_asian_samples)\n",
    "\n",
    "print(\"Number of samples in Balanced - Female:\", balanced_female_samples)\n",
    "print(\"Number of samples in Balanced - Male:\", balanced_male_samples)\n",
    "print(\"Total samples in Balanced:\", total_balanced)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the paths to the folders\n",
    "fairface_dataset_path = \"fairface_race_dataset\" # this is the FairFace dataset\n",
    "east_asian_val_folder = os.path.join(fairface_dataset_path, \"East_Asian_Validation\")\n",
    "balanced_val_folder = os.path.join(fairface_dataset_path, \"Balanced_Validation\")\n",
    "\n",
    "aaf_dataset_path = \"aaf_dataset\" # this is the All-Age-Faces dataset\n",
    "# # create training_pseudo\n",
    "\n",
    "train_folder = os.path.join(aaf_dataset_path, \"Unlabeled\") # this contains the training data from pseudo-labelling of all-asian-faces\n",
    "os.makedirs(train_folder, exist_ok=True)\n",
    "\n",
    "validation_folder = os.path.join(east_asian_val_folder) # validate on east asian faces from FairFace\n",
    "test_folder = os.path.join(balanced_val_folder) # test on a balanced dataset from FairFace\n",
    "\n",
    "\n",
    "# Create ImageFolder datasets\n",
    "val_dataset = ImageFolder(validation_folder, transform=transforms_val)\n",
    "test_dataset = ImageFolder(test_folder, transform=transforms_test)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Create DataLoader objects\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def show_samples(dataloader, dataset_name):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "    fig.suptitle(f\"Samples from {dataset_name} Dataset\")\n",
    "    for ax in axes.flatten():\n",
    "        ax.axis('off')\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        if i == 6:\n",
    "            break\n",
    "        ax = axes[i // 3, i % 3]\n",
    "        # Show the image using imshow function\n",
    "        imshow(inputs[0], title=f\"Label: {labels[0]}\")\n",
    "        ax.axis('on')\n",
    "\n",
    "# Show samples from training dataset\n",
    "show_samples(train_dataloader, \"Training\")\n",
    "\n",
    "# Show samples from validation dataset\n",
    "show_samples(val_dataloader, \"Validation\")\n",
    "\n",
    "# Show samples from test dataset\n",
    "show_samples(test_dataloader, \"Test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haohu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\haohu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\haohu\\AppData\\Local\\Temp\\ipykernel_10080\\928303400.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(pretrained_weights_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully and saved as classification_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haohu\\AppData\\Local\\Temp\\ipykernel_10080\\928303400.py:116: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(pretrained_weights_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Iteration 1/20 ---\n",
      "Unlabeled dataset size: 11345\n",
      "Processed 0 images\n",
      "FlexMatch thresholds of the two classes: tensor([0.0842, 0.6839])\n",
      "Number of pseudo-labeled data: Male 4339, Female 844 samples before balancing.\n",
      "Balanced pseudo-labeled samples: Male - 844, Female - 844\n",
      "Iteration 1, Epoch 1, Train Loss: 0.0464\n",
      "Iteration 1, Epoch 1, Validation Loss: 0.5597, Accuracy: 0.7723\n",
      "Iteration 1, Epoch 2, Train Loss: 0.0236\n",
      "Iteration 1, Epoch 2, Validation Loss: 0.5994, Accuracy: 0.7697\n",
      "Iteration 1, Epoch 3, Train Loss: 0.0106\n",
      "Iteration 1, Epoch 3, Validation Loss: 0.5390, Accuracy: 0.7735\n",
      "Iteration 1, Epoch 4, Train Loss: 0.0085\n",
      "Iteration 1, Epoch 4, Validation Loss: 0.5549, Accuracy: 0.7852\n",
      "Iteration 1, Epoch 5, Train Loss: 0.0101\n",
      "Iteration 1, Epoch 5, Validation Loss: 0.6159, Accuracy: 0.7529\n",
      "Early stopping triggered.\n",
      "--- Starting Iteration 2/20 ---\n",
      "Unlabeled dataset size: 11345\n",
      "Processed 10000 images\n",
      "FlexMatch thresholds of the two classes: tensor([0.2198, 0.4311])\n",
      "Number of pseudo-labeled data: Male 6890, Female 4147 samples before balancing.\n",
      "Balanced pseudo-labeled samples: Male - 4147, Female - 4147\n",
      "Iteration 2, Epoch 1, Train Loss: 0.2939\n",
      "Iteration 2, Epoch 1, Validation Loss: 1.0479, Accuracy: 0.6942\n",
      "Iteration 2, Epoch 2, Train Loss: 0.2026\n",
      "Iteration 2, Epoch 2, Validation Loss: 1.0814, Accuracy: 0.6839\n",
      "Iteration 2, Epoch 3, Train Loss: 0.1548\n",
      "Iteration 2, Epoch 3, Validation Loss: 1.1915, Accuracy: 0.6877\n",
      "Early stopping triggered.\n",
      "--- Starting Iteration 3/20 ---\n",
      "Unlabeled dataset size: 11345\n",
      "Processed 10000 images\n",
      "FlexMatch thresholds of the two classes: tensor([0.2402, 0.4036])\n",
      "Number of pseudo-labeled data: Male 6766, Female 4579 samples before balancing.\n",
      "Balanced pseudo-labeled samples: Male - 4579, Female - 4579\n",
      "Iteration 3, Epoch 1, Train Loss: 0.2056\n",
      "Iteration 3, Epoch 1, Validation Loss: 1.1135, Accuracy: 0.6826\n",
      "Iteration 3, Epoch 2, Train Loss: 0.1522\n",
      "Iteration 3, Epoch 2, Validation Loss: 1.2364, Accuracy: 0.6723\n",
      "Iteration 3, Epoch 3, Train Loss: 0.1395\n",
      "Iteration 3, Epoch 3, Validation Loss: 1.1441, Accuracy: 0.6781\n",
      "Early stopping triggered.\n",
      "--- Starting Iteration 4/20 ---\n",
      "Unlabeled dataset size: 11345\n",
      "Processed 10000 images\n",
      "FlexMatch thresholds of the two classes: tensor([0.2378, 0.4068])\n",
      "Number of pseudo-labeled data: Male 6803, Female 4542 samples before balancing.\n",
      "Balanced pseudo-labeled samples: Male - 4542, Female - 4542\n",
      "Iteration 4, Epoch 1, Train Loss: 0.1508\n",
      "Iteration 4, Epoch 1, Validation Loss: 1.1304, Accuracy: 0.6987\n",
      "Iteration 4, Epoch 2, Train Loss: 0.1311\n",
      "Iteration 4, Epoch 2, Validation Loss: 1.5119, Accuracy: 0.6516\n",
      "Iteration 4, Epoch 3, Train Loss: 0.1158\n",
      "Iteration 4, Epoch 3, Validation Loss: 2.0349, Accuracy: 0.6265\n",
      "Early stopping triggered.\n",
      "--- Starting Iteration 5/20 ---\n",
      "Unlabeled dataset size: 11345\n",
      "Processed 10000 images\n",
      "FlexMatch thresholds of the two classes: tensor([0.2488, 0.3927])\n",
      "Number of pseudo-labeled data: Male 6636, Female 4709 samples before balancing.\n",
      "Balanced pseudo-labeled samples: Male - 4709, Female - 4709\n",
      "Iteration 5, Epoch 1, Train Loss: 0.1335\n",
      "Iteration 5, Epoch 1, Validation Loss: 1.9004, Accuracy: 0.6342\n",
      "Iteration 5, Epoch 2, Train Loss: 0.1081\n",
      "Iteration 5, Epoch 2, Validation Loss: 2.2435, Accuracy: 0.6355\n",
      "Iteration 5, Epoch 3, Train Loss: 0.0960\n",
      "Iteration 5, Epoch 3, Validation Loss: 2.5625, Accuracy: 0.6284\n",
      "Early stopping triggered.\n",
      "--- Starting Iteration 6/20 ---\n",
      "Unlabeled dataset size: 11345\n",
      "Processed 10000 images\n",
      "FlexMatch thresholds of the two classes: tensor([0.2591, 0.3799])\n",
      "Number of pseudo-labeled data: Male 6482, Female 4863 samples before balancing.\n",
      "Balanced pseudo-labeled samples: Male - 4863, Female - 4863\n",
      "Iteration 6, Epoch 1, Train Loss: 0.1170\n",
      "Iteration 6, Epoch 1, Validation Loss: 2.4165, Accuracy: 0.6065\n",
      "Iteration 6, Epoch 2, Train Loss: 0.0937\n",
      "Iteration 6, Epoch 2, Validation Loss: 3.0099, Accuracy: 0.6168\n",
      "Iteration 6, Epoch 3, Train Loss: 0.0881\n",
      "Iteration 6, Epoch 3, Validation Loss: 2.7700, Accuracy: 0.6323\n",
      "Early stopping triggered.\n",
      "--- Starting Iteration 7/20 ---\n",
      "Unlabeled dataset size: 11345\n",
      "Processed 10000 images\n",
      "FlexMatch thresholds of the two classes: tensor([0.2550, 0.3850])\n",
      "Number of pseudo-labeled data: Male 6543, Female 4802 samples before balancing.\n",
      "Balanced pseudo-labeled samples: Male - 4802, Female - 4802\n",
      "Iteration 7, Epoch 1, Train Loss: 0.1088\n",
      "Iteration 7, Epoch 1, Validation Loss: 2.6431, Accuracy: 0.6065\n",
      "Iteration 7, Epoch 2, Train Loss: 0.0642\n",
      "Iteration 7, Epoch 2, Validation Loss: 2.3849, Accuracy: 0.6561\n",
      "Iteration 7, Epoch 3, Train Loss: 0.0565\n",
      "Iteration 7, Epoch 3, Validation Loss: 2.3169, Accuracy: 0.6310\n",
      "Iteration 7, Epoch 4, Train Loss: 0.0615\n",
      "Iteration 7, Epoch 4, Validation Loss: 1.9357, Accuracy: 0.6723\n",
      "Iteration 7, Epoch 5, Train Loss: 0.0410\n",
      "Iteration 7, Epoch 5, Validation Loss: 2.2989, Accuracy: 0.6335\n",
      "Iteration 7, Epoch 6, Train Loss: 0.0346\n",
      "Iteration 7, Epoch 6, Validation Loss: 2.7632, Accuracy: 0.6226\n",
      "Early stopping triggered.\n",
      "--- Starting Iteration 8/20 ---\n",
      "Unlabeled dataset size: 11345\n",
      "Processed 10000 images\n",
      "FlexMatch thresholds of the two classes: tensor([0.2545, 0.3856])\n",
      "Number of pseudo-labeled data: Male 6551, Female 4794 samples before balancing.\n",
      "Balanced pseudo-labeled samples: Male - 4794, Female - 4794\n",
      "Iteration 8, Epoch 1, Train Loss: 0.0593\n",
      "Iteration 8, Epoch 1, Validation Loss: 3.4550, Accuracy: 0.6032\n",
      "Iteration 8, Epoch 2, Train Loss: 0.0466\n",
      "Iteration 8, Epoch 2, Validation Loss: 2.3602, Accuracy: 0.6348\n",
      "Iteration 8, Epoch 3, Train Loss: 0.0534\n",
      "Iteration 8, Epoch 3, Validation Loss: 2.2907, Accuracy: 0.6677\n",
      "Iteration 8, Epoch 4, Train Loss: 0.0324\n",
      "Iteration 8, Epoch 4, Validation Loss: 2.8579, Accuracy: 0.6077\n",
      "Iteration 8, Epoch 5, Train Loss: 0.0326\n",
      "Iteration 8, Epoch 5, Validation Loss: 3.1234, Accuracy: 0.6213\n",
      "Early stopping triggered.\n",
      "--- Starting Iteration 9/20 ---\n",
      "Unlabeled dataset size: 11345\n",
      "Processed 10000 images\n",
      "FlexMatch thresholds of the two classes: tensor([0.2534, 0.3869])\n",
      "Number of pseudo-labeled data: Male 6567, Female 4778 samples before balancing.\n",
      "Balanced pseudo-labeled samples: Male - 4778, Female - 4778\n",
      "Iteration 9, Epoch 1, Train Loss: 0.0468\n",
      "Iteration 9, Epoch 1, Validation Loss: 2.3426, Accuracy: 0.6490\n",
      "Iteration 9, Epoch 2, Train Loss: 0.0390\n",
      "Iteration 9, Epoch 2, Validation Loss: 2.2601, Accuracy: 0.6484\n",
      "Iteration 9, Epoch 3, Train Loss: 0.0448\n",
      "Iteration 9, Epoch 3, Validation Loss: 2.8164, Accuracy: 0.6206\n",
      "Iteration 9, Epoch 4, Train Loss: 0.0220\n",
      "Iteration 9, Epoch 4, Validation Loss: 1.7958, Accuracy: 0.6787\n",
      "Iteration 9, Epoch 5, Train Loss: 0.0234\n",
      "Iteration 9, Epoch 5, Validation Loss: 3.0925, Accuracy: 0.6181\n",
      "Iteration 9, Epoch 6, Train Loss: 0.0250\n",
      "Iteration 9, Epoch 6, Validation Loss: 3.2711, Accuracy: 0.6194\n",
      "Early stopping triggered.\n",
      "--- Starting Iteration 10/20 ---\n",
      "Unlabeled dataset size: 11345\n",
      "Processed 10000 images\n",
      "FlexMatch thresholds of the two classes: tensor([0.2546, 0.3855])\n",
      "Number of pseudo-labeled data: Male 6549, Female 4796 samples before balancing.\n",
      "Balanced pseudo-labeled samples: Male - 4796, Female - 4796\n",
      "Iteration 10, Epoch 1, Train Loss: 0.0321\n",
      "Iteration 10, Epoch 1, Validation Loss: 2.9835, Accuracy: 0.6323\n",
      "Iteration 10, Epoch 2, Train Loss: 0.0267\n",
      "Iteration 10, Epoch 2, Validation Loss: 2.5193, Accuracy: 0.6439\n",
      "Iteration 10, Epoch 3, Train Loss: 0.0260\n",
      "Iteration 10, Epoch 3, Validation Loss: 3.5168, Accuracy: 0.6542\n",
      "Iteration 10, Epoch 4, Train Loss: 0.0142\n",
      "Iteration 10, Epoch 4, Validation Loss: 2.7212, Accuracy: 0.6587\n",
      "Early stopping triggered.\n",
      "--- Starting Iteration 11/20 ---\n",
      "Unlabeled dataset size: 11345\n",
      "Processed 10000 images\n",
      "FlexMatch thresholds of the two classes: tensor([0.2525, 0.3880])\n",
      "Number of pseudo-labeled data: Male 6580, Female 4765 samples before balancing.\n",
      "Balanced pseudo-labeled samples: Male - 4765, Female - 4765\n",
      "Iteration 11, Epoch 1, Train Loss: 0.0332\n",
      "Iteration 11, Epoch 1, Validation Loss: 3.5948, Accuracy: 0.6206\n",
      "Iteration 11, Epoch 2, Train Loss: 0.0217\n",
      "Iteration 11, Epoch 2, Validation Loss: 3.0753, Accuracy: 0.6310\n",
      "Iteration 11, Epoch 3, Train Loss: 0.0173\n",
      "Iteration 11, Epoch 3, Validation Loss: 2.4789, Accuracy: 0.6581\n",
      "Iteration 11, Epoch 4, Train Loss: 0.0108\n",
      "Iteration 11, Epoch 4, Validation Loss: 2.9471, Accuracy: 0.6529\n",
      "Iteration 11, Epoch 5, Train Loss: 0.0144\n",
      "Iteration 11, Epoch 5, Validation Loss: 3.4679, Accuracy: 0.6232\n",
      "Early stopping triggered.\n",
      "--- Starting Iteration 12/20 ---\n",
      "Unlabeled dataset size: 11345\n",
      "Processed 10000 images\n",
      "FlexMatch thresholds of the two classes: tensor([0.2540, 0.3862])\n",
      "Number of pseudo-labeled data: Male 6558, Female 4787 samples before balancing.\n",
      "Balanced pseudo-labeled samples: Male - 4787, Female - 4787\n",
      "Iteration 12, Epoch 1, Train Loss: 0.0146\n",
      "Iteration 12, Epoch 1, Validation Loss: 3.1599, Accuracy: 0.6600\n",
      "Iteration 12, Epoch 2, Train Loss: 0.0184\n",
      "Iteration 12, Epoch 2, Validation Loss: 3.4808, Accuracy: 0.6245\n",
      "Iteration 12, Epoch 3, Train Loss: 0.0128\n",
      "Iteration 12, Epoch 3, Validation Loss: 3.7475, Accuracy: 0.6232\n",
      "Early stopping triggered.\n",
      "--- Starting Iteration 13/20 ---\n",
      "Unlabeled dataset size: 11345\n",
      "Processed 10000 images\n",
      "FlexMatch thresholds of the two classes: tensor([0.2543, 0.3859])\n",
      "Number of pseudo-labeled data: Male 6554, Female 4791 samples before balancing.\n",
      "Balanced pseudo-labeled samples: Male - 4791, Female - 4791\n",
      "Iteration 13, Epoch 1, Train Loss: 0.0221\n",
      "Iteration 13, Epoch 1, Validation Loss: 3.5039, Accuracy: 0.6284\n",
      "Iteration 13, Epoch 2, Train Loss: 0.0163\n",
      "Iteration 13, Epoch 2, Validation Loss: 2.9542, Accuracy: 0.6381\n",
      "Iteration 13, Epoch 3, Train Loss: 0.0121\n",
      "Iteration 13, Epoch 3, Validation Loss: 2.6967, Accuracy: 0.6445\n",
      "Iteration 13, Epoch 4, Train Loss: 0.0168\n",
      "Iteration 13, Epoch 4, Validation Loss: 3.7101, Accuracy: 0.6142\n",
      "Iteration 13, Epoch 5, Train Loss: 0.0218\n",
      "Iteration 13, Epoch 5, Validation Loss: 2.3949, Accuracy: 0.6626\n",
      "Iteration 13, Epoch 6, Train Loss: 0.0111\n",
      "Iteration 13, Epoch 6, Validation Loss: 2.8383, Accuracy: 0.6406\n",
      "Iteration 13, Epoch 7, Train Loss: 0.0073\n",
      "Iteration 13, Epoch 7, Validation Loss: 2.6757, Accuracy: 0.6477\n",
      "Early stopping triggered.\n",
      "--- Starting Iteration 14/20 ---\n",
      "Unlabeled dataset size: 11345\n",
      "Processed 10000 images\n",
      "FlexMatch thresholds of the two classes: tensor([0.2544, 0.3858])\n",
      "Number of pseudo-labeled data: Male 6553, Female 4792 samples before balancing.\n",
      "Balanced pseudo-labeled samples: Male - 4792, Female - 4792\n",
      "Iteration 14, Epoch 1, Train Loss: 0.0025\n",
      "Iteration 14, Epoch 1, Validation Loss: 2.8177, Accuracy: 0.6484\n",
      "Iteration 14, Epoch 2, Train Loss: 0.0069\n",
      "Iteration 14, Epoch 2, Validation Loss: 2.4798, Accuracy: 0.6839\n",
      "Iteration 14, Epoch 3, Train Loss: 0.0063\n",
      "Iteration 14, Epoch 3, Validation Loss: 1.5981, Accuracy: 0.7245\n",
      "Iteration 14, Epoch 4, Train Loss: 0.0075\n",
      "Iteration 14, Epoch 4, Validation Loss: 2.5549, Accuracy: 0.6652\n",
      "Iteration 14, Epoch 5, Train Loss: 0.0119\n",
      "Iteration 14, Epoch 5, Validation Loss: 2.5658, Accuracy: 0.6645\n",
      "Early stopping triggered.\n",
      "--- Starting Iteration 15/20 ---\n",
      "Unlabeled dataset size: 11345\n",
      "Processed 10000 images\n",
      "FlexMatch thresholds of the two classes: tensor([0.2545, 0.3856])\n",
      "Number of pseudo-labeled data: Male 6551, Female 4794 samples before balancing.\n",
      "Balanced pseudo-labeled samples: Male - 4794, Female - 4794\n",
      "Iteration 15, Epoch 1, Train Loss: 0.0089\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 246\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m val_dataloader:\n\u001b[1;32m--> 246\u001b[0m         inputs, labels \u001b[38;5;241m=\u001b[39m \u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    247\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m    248\u001b[0m         val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Balanced Training dataset using Method 1 (Substitute pseudo-labeled training set with the newly pseudo-labeled from the current model)\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np \n",
    "import requests\n",
    "import psutil\n",
    "\n",
    "# Define the paths to the folders\n",
    "fairface_dataset_path = \"fairface_race_dataset\" # this is the FairFace dataset\n",
    "east_asian_val_folder = os.path.join(fairface_dataset_path, \"East_Asian_Validation\")\n",
    "balanced_val_folder = os.path.join(fairface_dataset_path, \"Balanced_Validation\")\n",
    "aaf_dataset_path = \"aaf_dataset\" # this is the All-Age-Faces dataset\n",
    "# # create training_pseudo\n",
    "train_folder = os.path.join(aaf_dataset_path, \"Training_pseudo\") # this contains the training data from pseudo-labelling of all-asian-faces\n",
    "aaf_training_female_path= os.path.join(train_folder, \"female\")\n",
    "aaf_training_male_path = os.path.join(train_folder, \"male\")\n",
    "\n",
    "os.makedirs(train_folder, exist_ok=True)\n",
    "validation_folder = os.path.join(east_asian_val_folder) # validate on east asian faces from FairFace\n",
    "test_folder = os.path.join(balanced_val_folder) # test on a balanced dataset from FairFace\n",
    "\n",
    "unlabeled_folder = os.path.join(aaf_dataset_path, \"Unlabeled\") # pseudo-labeling path of the training dataset of AAF\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(aaf_dataset_path, exist_ok=True)\n",
    "os.makedirs(train_folder, exist_ok=True)\n",
    "os.makedirs(aaf_training_female_path, exist_ok=True)\n",
    "os.makedirs(aaf_training_male_path, exist_ok=True)\n",
    "# Define the transformations\n",
    "transforms_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transforms_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transforms_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "# Load validation dataset\n",
    "val_dataset = ImageFolder(validation_folder, transform=transforms_val)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "\n",
    "# Define model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Load pre-trained ResNet model CNN-based Face-Gender-Classification PyTorch model \n",
    "model = models.resnet18(pretrained=True)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 2)  # Output layer for binary classification\n",
    "# Load pre-trained weights from .pth file\n",
    "pretrained_weights_path = 'classification_model.pth'\n",
    "model.load_state_dict(torch.load(pretrained_weights_path, map_location=device))\n",
    "model = model.to(device)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define the non-linear mapping function (e.g., convex function)\n",
    "def non_linear_mapping(x):\n",
    "    return x / (2 - x)  # Convex function for threshold adjustment\n",
    "\n",
    "# Initialize class-specific thresholds and counts\n",
    "num_classes = 2  # Male and Female\n",
    "base_threshold = 0.95  # Base confidence threshold for pseudo-labeling\n",
    "iterations = 20  # Number of self-training iterations\n",
    "num_epochs = 10  # Epochs per iteration\n",
    "batch_size = 16\n",
    "patience = 2  # Early stopping patience\n",
    "# Define model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the URL for the file\n",
    "url = \"https://postechackr-my.sharepoint.com/:u:/g/personal/dongbinna_postech_ac_kr/EVd9bFWzqztMrXRDdNnCHQkBsHaM4n5_1q1fue77vtQVtw?download=1\"\n",
    "# Define the path where the file will be saved\n",
    "output_path = \"classification_model.pth\"\n",
    "# Send a GET request to download the file\n",
    "response = requests.get(url)\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    with open(output_path, \"wb\") as file:\n",
    "        file.write(response.content)  # Write the content of the response to the file\n",
    "    print(f\"File downloaded successfully and saved as {output_path}\")\n",
    "else:\n",
    "    print(f\"Failed to download file. Status code: {response.status_code}\")\n",
    "\n",
    "# Load pre-trained ResNet model CNN-based Face-Gender-Classification PyTorch model \n",
    "model = models.resnet18(pretrained=True)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 2)  # Output layer for binary classification\n",
    "pretrained_weights_path = 'classification_model.pth'\n",
    "model.load_state_dict(torch.load(pretrained_weights_path, map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Initialize class thresholds and counts\n",
    "class_thresholds = torch.ones(num_classes) * base_threshold  # Initial thresholds\n",
    "class_counts = torch.zeros(num_classes)  # Track pseudo-labeled samples per class\n",
    "\n",
    "\n",
    "for iteration in range(14, iterations):\n",
    "    print(f\"--- Starting Iteration {iteration + 1}/{iterations} ---\")\n",
    "    print(\"Unlabeled dataset size:\", len(os.listdir(unlabeled_folder)))\n",
    "\n",
    "    # 1. Clean the training folder\n",
    "    shutil.rmtree(train_folder, ignore_errors=True)\n",
    "    os.makedirs(train_folder, exist_ok=True)\n",
    "    os.makedirs(aaf_training_female_path, exist_ok=True)\n",
    "    os.makedirs(aaf_training_male_path, exist_ok=True)\n",
    "\n",
    "    # 2. Pseudo-labeling using FlexMatch\n",
    "    model.eval()\n",
    "    class_counts = torch.zeros(num_classes)  # Reset counts for each iteration\n",
    "    new_samples = 0\n",
    "\n",
    "    for unlabeled_data in os.listdir(unlabeled_folder):\n",
    "        img_path = os.path.join(unlabeled_folder, unlabeled_data)\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        input = transforms_val(img).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input)\n",
    "            probs = F.softmax(output, dim=1)\n",
    "            max_probs, preds = torch.max(probs, dim=1)\n",
    "\n",
    "            # Apply FlexMatch thresholds\n",
    "            for c in range(num_classes):\n",
    "                mask = (preds == c) & (max_probs > class_thresholds[c])\n",
    "                if mask.any():\n",
    "                    class_counts[c] += mask.sum().item()\n",
    "                    # Save pseudo-labeled data\n",
    "                    if c == 0:\n",
    "                        shutil.copy(img_path, os.path.join(aaf_training_female_path, unlabeled_data))\n",
    "                    elif c == 1:\n",
    "                        shutil.copy(img_path, os.path.join(aaf_training_male_path, unlabeled_data))\n",
    "                    new_samples += 1\n",
    "\n",
    "        if new_samples % 10000 == 0:\n",
    "            print(f\"Processed {new_samples} images\")\n",
    "\n",
    "    # 3. Update class thresholds based on learning status\n",
    "    normalized_counts = class_counts / class_counts.sum()\n",
    "    for c in range(num_classes):\n",
    "        class_thresholds[c] = non_linear_mapping(normalized_counts[c]) * base_threshold\n",
    "\n",
    "    print(\"FlexMatch thresholds of the two classes:\", class_thresholds)\n",
    "\n",
    "    # 4. Balance the training dataset\n",
    "    male_samples = os.listdir(aaf_training_male_path) if os.path.exists(aaf_training_male_path) else []\n",
    "    female_samples = os.listdir(aaf_training_female_path) if os.path.exists(aaf_training_female_path) else []\n",
    "    num_male = len(male_samples)\n",
    "    num_female = len(female_samples)\n",
    "    target_size = min(num_male, num_female)\n",
    "    print(f\"Number of pseudo-labeled data: Male {num_male}, Female {num_female} samples before balancing.\")\n",
    "\n",
    "    if num_male > target_size:\n",
    "        excess_male_samples = np.random.choice(male_samples, num_male - target_size, replace=False)\n",
    "        for sample in excess_male_samples:\n",
    "            os.remove(os.path.join(aaf_training_male_path, sample))\n",
    "    elif num_female > target_size:\n",
    "        excess_female_samples = np.random.choice(female_samples, num_female - target_size, replace=False)\n",
    "        for sample in excess_female_samples:\n",
    "            os.remove(os.path.join(aaf_training_female_path, sample))\n",
    "\n",
    "    # Recount samples after balancing\n",
    "    num_male = len(os.listdir(aaf_training_male_path))\n",
    "    num_female = len(os.listdir(aaf_training_female_path))\n",
    "    print(f\"Balanced pseudo-labeled samples: Male - {num_male}, Female - {num_female}\")\n",
    "\n",
    "    # Reload training dataset with new pseudo-labeled samples\n",
    "    train_dataset = datasets.ImageFolder(train_folder, transform=transforms_train)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    # 5. Training with early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    wait = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, labels in train_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            max_probs, preds = torch.max(probs, dim=1)\n",
    "\n",
    "            # Supervised loss\n",
    "            supervised_loss = criterion(outputs, labels)\n",
    "\n",
    "            # Unsupervised loss (FlexMatch)\n",
    "            unsupervised_loss = 0.0\n",
    "            for c in range(num_classes):\n",
    "                mask = (preds == c) & (max_probs > class_thresholds[c])\n",
    "                if mask.any():\n",
    "                    unsupervised_loss += F.cross_entropy(outputs[mask], preds[mask])\n",
    "\n",
    "            # Total loss\n",
    "            loss = supervised_loss + unsupervised_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_dataset)\n",
    "        print(f\"Iteration {iteration + 1}, Epoch {epoch + 1}, Train Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_dataloader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, labels).item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        val_loss /= len(val_dataset)\n",
    "        val_accuracy = correct / total\n",
    "        print(f\"Iteration {iteration + 1}, Epoch {epoch + 1}, Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            wait = 0\n",
    "            torch.save(model.state_dict(), f\"asian_faces_FlexMatch_{iteration + 1}_best.pth\")\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience: \n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "# Save the final model\n",
    "torch.save(model.state_dict(), f'asian_facesFlexMatch_{iteration+1}_final.pth')\n",
    "print(\"Training completed with FlexMatch and CPL.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class names: ['female', 'male']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haohu\\AppData\\Local\\Temp\\ipykernel_15388\\2882410176.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f'asian_faces_FlexMatch_{j}_best.pth', map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1\n",
      "[Prediction Result Examples]\n",
      "[Test] Loss: 0.6834 Acc: 53.2903% Time: 19.1626s\n",
      "[Test] Male Accuracy: 86.6152%\n",
      "[Test] Female Accuracy: 19.7930%\n",
      "Iteration:  2\n",
      "[Prediction Result Examples]\n",
      "[Test] Loss: 0.8026 Acc: 37.1613% Time: 21.9370s\n",
      "[Test] Male Accuracy: 38.3526%\n",
      "[Test] Female Accuracy: 35.9638%\n",
      "Iteration:  3\n",
      "[Prediction Result Examples]\n",
      "[Test] Loss: 0.7358 Acc: 46.2581% Time: 25.5070s\n",
      "[Test] Male Accuracy: 19.0476%\n",
      "[Test] Female Accuracy: 73.6093%\n",
      "Iteration:  4\n",
      "[Prediction Result Examples]\n",
      "[Test] Loss: 0.7091 Acc: 48.9032% Time: 23.5176s\n",
      "[Test] Male Accuracy: 83.6551%\n",
      "[Test] Female Accuracy: 13.9715%\n",
      "Iteration:  5\n",
      "[Prediction Result Examples]\n",
      "[Test] Loss: 0.6639 Acc: 56.1936% Time: 26.6687s\n",
      "[Test] Male Accuracy: 17.2458%\n",
      "[Test] Female Accuracy: 95.3428%\n",
      "Iteration:  6\n",
      "[Prediction Result Examples]\n",
      "[Test] Loss: 0.7160 Acc: 50.8387% Time: 35.0273s\n",
      "[Test] Male Accuracy: 9.0090%\n",
      "[Test] Female Accuracy: 92.8849%\n",
      "Iteration:  7\n",
      "[Prediction Result Examples]\n",
      "[Test] Loss: 0.6845 Acc: 53.8710% Time: 28.4953s\n",
      "[Test] Male Accuracy: 96.1390%\n",
      "[Test] Female Accuracy: 11.3842%\n",
      "Iteration:  8\n",
      "[Prediction Result Examples]\n",
      "[Test] Loss: 0.7251 Acc: 45.5484% Time: 23.2575s\n",
      "[Test] Male Accuracy: 36.8082%\n",
      "[Test] Female Accuracy: 54.3338%\n",
      "Iteration:  9\n",
      "[Prediction Result Examples]\n",
      "[Test] Loss: 0.6176 Acc: 66.9032% Time: 27.2989s\n",
      "[Test] Male Accuracy: 58.3012%\n",
      "[Test] Female Accuracy: 75.5498%\n",
      "Iteration:  10\n",
      "[Prediction Result Examples]\n",
      "[Test] Loss: 0.6510 Acc: 62.0000% Time: 26.8128s\n",
      "[Test] Male Accuracy: 62.8057%\n",
      "[Test] Female Accuracy: 61.1902%\n",
      "Iteration:  11\n",
      "[Prediction Result Examples]\n",
      "[Test] Loss: 0.8583 Acc: 49.8710% Time: 24.4772s\n",
      "[Test] Male Accuracy: 0.0000%\n",
      "[Test] Female Accuracy: 100.0000%\n",
      "Iteration:  12\n",
      "[Prediction Result Examples]\n",
      "[Test] Loss: 0.7089 Acc: 49.1613% Time: 25.8344s\n",
      "[Test] Male Accuracy: 51.9949%\n",
      "[Test] Female Accuracy: 46.3131%\n",
      "Iteration:  13\n",
      "[Prediction Result Examples]\n",
      "[Test] Loss: 0.7836 Acc: 42.3871% Time: 25.0718s\n",
      "[Test] Male Accuracy: 30.5019%\n",
      "[Test] Female Accuracy: 54.3338%\n",
      "Iteration:  14\n",
      "[Prediction Result Examples]\n",
      "[Test] Loss: 0.6240 Acc: 68.9677% Time: 27.9971s\n",
      "[Test] Male Accuracy: 59.9743%\n",
      "[Test] Female Accuracy: 78.0078%\n"
     ]
    }
   ],
   "source": [
    "# Test Phase\n",
    "# import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "def imshow(input, title):\n",
    "    # Convert torch.Tensor to numpy array\n",
    "    input = input.numpy().transpose((1, 2, 0))\n",
    "    # Undo image normalization\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    input = std * input + mean\n",
    "    input = np.clip(input, 0, 1)\n",
    "    # Display images\n",
    "    plt.imshow(input)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "transforms_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "aaf_dataset_path = \"./fairface_race_dataset\"\n",
    "# Load the model weights\n",
    "balanced_test_folder = os.path.join(aaf_dataset_path, \"East_Asian_Validation\")\n",
    "# Now the model is ready for inference\n",
    "batch_size = 16\n",
    "balanced_test_dataset = ImageFolder(balanced_test_folder, transform=transforms_val)\n",
    "balanced_test_dataloader = DataLoader(balanced_test_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "class_names = balanced_test_dataset.classes\n",
    "print('Class names:', class_names)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_features = model.fc.in_features\n",
    "results = []\n",
    "for j in range(1, 15):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    state_dict = torch.load(f'asian_faces_FlexMatch_{j}_best.pth', map_location=device)\n",
    "    filtered_state_dict = {k: v for k, v in state_dict.items() if not k.startswith(\"fc.\")}\n",
    "    model.load_state_dict(filtered_state_dict, strict=False)  # strict=False ignores missing layers\n",
    "    # binary classification\n",
    "    model.fc = nn.Linear(num_features, 2)\n",
    "    \n",
    "    print('Iteration: ', j)\n",
    "    # Move the model to the device\n",
    "    model.to(device)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    male_corrects = 0\n",
    "    male_total = 0\n",
    "    female_corrects = 0\n",
    "    female_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.\n",
    "        running_corrects = 0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(balanced_test_dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            male_mask = labels == class_names.index('male')\n",
    "            female_mask = labels == class_names.index('female')\n",
    "\n",
    "            male_corrects += torch.sum(preds[male_mask] == labels[male_mask])\n",
    "            male_total += torch.sum(male_mask)\n",
    "\n",
    "            female_corrects += torch.sum(preds[female_mask] == labels[female_mask])\n",
    "            female_total += torch.sum(female_mask)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if i == 0:\n",
    "                print('[Prediction Result Examples]')\n",
    "                pred_labels = [class_names[x] for x in preds]\n",
    "                images = torchvision.utils.make_grid(inputs[:8])\n",
    "                # imshow(images.cpu(), title=pred_labels)\n",
    "\n",
    "        male_acc = male_corrects.double() / male_total * 100. if male_total > 0 else 0\n",
    "        female_acc = female_corrects.double() / female_total * 100. if female_total > 0 else 0\n",
    "\n",
    "        epoch_loss = running_loss / len(balanced_test_dataset)\n",
    "        epoch_acc = running_corrects / len(balanced_test_dataset) * 100.\n",
    "        print('[Test] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch_loss, epoch_acc,\n",
    "                                                                            time.time() - start_time))\n",
    "        print('[Test] Male Accuracy: {:.4f}%'.format(male_acc))\n",
    "        print('[Test] Female Accuracy: {:.4f}%'.format(female_acc))\n",
    "        # save male, female accuracy and iteration to csv\n",
    "        results.append([j, epoch_loss, epoch_acc, male_acc.item(), female_acc.item()])\n",
    "\n",
    "# Save results to CSV\n",
    "df = pd.DataFrame(results, columns=[\"Iteration\", \"Loss\", \"Accuracy\", \"Male Accuracy\", \"Female Accuracy\"])\n",
    "df.to_csv(\"aaf_flexmatch_validation_results.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Phase\n",
    "# import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "def imshow(input, title):\n",
    "    # Convert torch.Tensor to numpy array\n",
    "    input = input.numpy().transpose((1, 2, 0))\n",
    "    # Undo image normalization\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    input = std * input + mean\n",
    "    input = np.clip(input, 0, 1)\n",
    "    # Display images\n",
    "    plt.imshow(input)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "transforms_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "aaf_dataset_path = \"./fairface_race_dataset\"\n",
    "# Load the model weights\n",
    "balanced_test_folder = os.path.join(aaf_dataset_path, \"East_Asian_Validation\")\n",
    "# Now the model is ready for inference\n",
    "batch_size = 16\n",
    "balanced_test_dataset = ImageFolder(balanced_test_folder, transform=transforms_val)\n",
    "balanced_test_dataloader = DataLoader(balanced_test_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "class_names = balanced_test_dataset.classes\n",
    "print('Class names:', class_names)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_features = model.fc.in_features\n",
    "results = []\n",
    "for j in range(1, 11):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    state_dict = torch.load(f'DST_0.6_{j}_balanced_best.pth', map_location=device)\n",
    "    filtered_state_dict = {k: v for k, v in state_dict.items() if not k.startswith(\"fc.\")}\n",
    "    model.load_state_dict(filtered_state_dict, strict=False)  # strict=False ignores missing layers\n",
    "    # binary classification\n",
    "    model.fc = nn.Linear(num_features, 2)\n",
    "    \n",
    "    print('Iteration: ', j)\n",
    "    # Move the model to the device\n",
    "    model.to(device)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    male_corrects = 0\n",
    "    male_total = 0\n",
    "    female_corrects = 0\n",
    "    female_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.\n",
    "        running_corrects = 0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(balanced_test_dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            male_mask = labels == class_names.index('male')\n",
    "            female_mask = labels == class_names.index('female')\n",
    "\n",
    "            male_corrects += torch.sum(preds[male_mask] == labels[male_mask])\n",
    "            male_total += torch.sum(male_mask)\n",
    "\n",
    "            female_corrects += torch.sum(preds[female_mask] == labels[female_mask])\n",
    "            female_total += torch.sum(female_mask)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if i == 0:\n",
    "                print('[Prediction Result Examples]')\n",
    "                pred_labels = [class_names[x] for x in preds]\n",
    "                images = torchvision.utils.make_grid(inputs[:8])\n",
    "                # imshow(images.cpu(), title=pred_labels)\n",
    "\n",
    "        male_acc = male_corrects.double() / male_total * 100. if male_total > 0 else 0\n",
    "        female_acc = female_corrects.double() / female_total * 100. if female_total > 0 else 0\n",
    "\n",
    "        epoch_loss = running_loss / len(balanced_test_dataset)\n",
    "        epoch_acc = running_corrects / len(balanced_test_dataset) * 100.\n",
    "        print('[Validation] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch_loss, epoch_acc,\n",
    "                                                                            time.time() - start_time))\n",
    "        print('[Validation] Male Accuracy: {:.4f}%'.format(male_acc))\n",
    "        print('[Validation] Female Accuracy: {:.4f}%'.format(female_acc))\n",
    "        # save male, female accuracy and iteration to csv\n",
    "        results.append([j, epoch_loss, epoch_acc, male_acc.item(), female_acc.item()])\n",
    "\n",
    "# Save results to CSV\n",
    "df = pd.DataFrame(results, columns=[\"Iteration\", \"Loss\", \"Accuracy\", \"Male Accuracy\", \"Female Accuracy\"])\n",
    "df.to_csv(\"aaf_flexmatch_validation_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Phase\n",
    "# import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "def imshow(input, title):\n",
    "    # Convert torch.Tensor to numpy array\n",
    "    input = input.numpy().transpose((1, 2, 0))\n",
    "    # Undo image normalization\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    input = std * input + mean\n",
    "    input = np.clip(input, 0, 1)\n",
    "    # Display images\n",
    "    plt.imshow(input)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "transforms_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "aaf_dataset_path = \"./gender_classification_dataset_asia\"\n",
    "# Load the model weights\n",
    "balanced_test_folder = os.path.join(aaf_dataset_path, \"Test_balanced\")\n",
    "# Now the model is ready for inference\n",
    "batch_size = 16\n",
    "balanced_test_dataset = ImageFolder(balanced_test_folder, transform=transforms_val)\n",
    "balanced_test_dataloader = DataLoader(balanced_test_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "class_names = balanced_test_dataset.classes\n",
    "print('Class names:', class_names)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 2)  # Output layer for binary classification\n",
    "\n",
    "for j in range(1, 13):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    state_dict = torch.load(f'method_1_0.9_{j}_balanced_best.pth', map_location=device)\n",
    "    model.load_state_dict(state_dict) \n",
    "    print('Iteration: ', j)\n",
    "    # Move the model to the device\n",
    "    model.to(device)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    male_corrects = 0\n",
    "    male_total = 0\n",
    "    female_corrects = 0\n",
    "    female_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.\n",
    "        running_corrects = 0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(balanced_test_dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            male_mask = labels == class_names.index('male')\n",
    "            female_mask = labels == class_names.index('female')\n",
    "\n",
    "            male_corrects += torch.sum(preds[male_mask] == labels[male_mask])\n",
    "            male_total += torch.sum(male_mask)\n",
    "\n",
    "            female_corrects += torch.sum(preds[female_mask] == labels[female_mask])\n",
    "            female_total += torch.sum(female_mask)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if i == 0:\n",
    "                # print('[Prediction Result Examples]')\n",
    "                pred_labels = [class_names[x] for x in preds]\n",
    "                images = torchvision.utils.make_grid(inputs[:8])\n",
    "                # imshow(images.cpu(), title=pred_labels)\n",
    "\n",
    "        male_acc = male_corrects.double() / male_total * 100. if male_total > 0 else 0\n",
    "        female_acc = female_corrects.double() / female_total * 100. if female_total > 0 else 0\n",
    "\n",
    "        epoch_loss = running_loss / len(balanced_test_dataset)\n",
    "        epoch_acc = running_corrects / len(balanced_test_dataset) * 100.\n",
    "        print('[Test] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch_loss, epoch_acc,\n",
    "                                                                            time.time() - start_time))\n",
    "        print('[Test] Male Accuracy: {:.4f}%'.format(male_acc))\n",
    "        print('[Test] Female Accuracy: {:.4f}%'.format(female_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to self_training_final_model-0.6_5_best.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.save(model.state_dict(), 'self_training_unbalanced_female_model-0.6_5_best.pth')\n",
    "print(\"Saved PyTorch Model State to self_training_final_model-0.6_5_best.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "\n",
    "\n",
    "# Define transforms for training and validation data\n",
    "transforms_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transforms_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transforms_unlabeled = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transforms_val_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
